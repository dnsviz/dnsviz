#!/usr/bin/env python

import codecs
import collections
import getopt
import json
import logging
import sys
import multiprocessing
import multiprocessing.managers
import threading
import time

import dns.name

from dnsviz.analysis import Analyst, DomainNameAnalysis, get_client_addresses, NetworkConnectivityException, _resolver
import dnsviz.format as fmt

logger = logging.getLogger('dnsviz.analysis')

#XXX this is a hack required for inter-process sharing of dns.name.Name
# instances using multiprocess
def _setattr_dummy(self, name, value):
    return super(dns.name.Name, self).__setattr__(name, value)
dns.name.Name.__setattr__ = _setattr_dummy

def _analyze((cls, name, dlv_domain, client_ipv4, client_ipv6, ceiling, cache, cache_lock)):
    if ceiling is not None and name.is_subdomain(ceiling):
        c = ceiling
    else:
        c = name
    try:
        a = cls(name, dlv_domain=dlv_domain, client_ipv4=client_ipv4, client_ipv6=client_ipv6, ceiling=c, analysis_cache=cache, analysis_cache_lock=cache_lock)
        return a.analyze()
    except:
        logger.exception('Error analyzing %s' % name.canonicalize().to_text())

class BulkAnalyst(object):
    def __init__(self, ceiling, dlv_domain):
        self.client_ipv4, self.client_ipv6 = get_client_addresses()
        if self.client_ipv4 is None and self.client_ipv6 is None:
            raise NetworkConnectivityException('No network interfaces available for analysis!')
        self.ceiling = ceiling
        self.dlv_domain = dlv_domain

        self.cache = {}
        self.cache_lock = threading.Lock()

    def analyze(self, names):
        name_objs = []
        for name in names:
            name_objs.append(_analyze((Analyst, name, self.dlv_domain, self.client_ipv4, self.client_ipv6, self.ceiling, self.cache, self.cache_lock)))
        return name_objs

class MultiProcessAnalyst(Analyst):
    analysis_model = DomainNameAnalysis

    def refresh_dependency_references(self, name_obj, trace=None):
        if trace is None:
            trace = []

        if name_obj.name in trace:
            return

        if name_obj.parent is not None:
            self.refresh_dependency_references(name_obj.parent, trace+[name_obj.name])
        if name_obj.dlv_parent is not None:
            self.refresh_dependency_references(name_obj.dlv_parent, trace+[name_obj.name])

        # loop until all deps have been added
        for cname in name_obj.cname_targets:
            while name_obj.cname_targets[cname] is None:
                try:
                    name_obj.cname_targets[cname] = self.analysis_cache[cname]
                except KeyError:
                    time.sleep(1)
            self.refresh_dependency_references(name_obj.cname_targets[cname], trace+[name_obj.name])
        for dname in name_obj.dname_targets:
            while name_obj.dname_targets[dname] is None:
                try:
                    name_obj.dname_targets[dname] = self.analysis_cache[dname]
                except KeyError:
                    time.sleep(1)
            self.refresh_dependency_references(name_obj.dname_targets[dname], trace+[name_obj.name])
        for signer in name_obj.external_signers:
            while name_obj.external_signers[signer] is None:
                try:
                    name_obj.external_signers[signer] = self.analysis_cache[signer]
                except KeyError:
                    time.sleep(1)
            self.refresh_dependency_references(name_obj.external_signers[signer], trace+[name_obj.name])
        if self.follow_ns:
            for ns in name_obj.ns_dependencies:
                while name_obj.ns_dependencies[ns] is None:
                    try:
                        name_obj.ns_dependencies[ns] = self.analysis_cache[ns]
                    except KeyError:
                        time.sleep(1)
                self.refresh_dependency_references(name_obj.ns_dependencies[ns], trace+[name_obj.name])

    def analyze(self):
        name_obj = super(MultiProcessAnalyst, self).analyze()
        if not self.trace:
            self.refresh_dependency_references(name_obj)
        return name_obj

class ParallelAnalyst(BulkAnalyst):
    def __init__(self, ceiling, dlv_domain, processes):
        super(ParallelAnalyst, self).__init__(ceiling, dlv_domain)
        self.manager = multiprocessing.managers.SyncManager()
        self.manager.start()

        self.processes = processes

        self.cache = self.manager.dict()
        self.cache_lock = self.manager.Lock()

    def analyze(self, names):
        pool = multiprocessing.Pool(self.processes)

        def _name_to_args_iter(names):
            for name in names:
                yield (MultiProcessAnalyst, name, self.dlv_domain, self.client_ipv4, self.client_ipv6, self.ceiling, self.cache, self.cache_lock)

        return pool.map(_analyze, _name_to_args_iter(names))

def usage():
    sys.stderr.write('''Usage: %s [ options ] ( -f <filename> | <domain name> [... ] )

Options:
    -f <filename>  - read names from a file (one name per line), instead of from command line
    -d <level>     - set debug level to a value from 0 to 3, with increasing verbosity (default: 1 or WARNING)
    -l <dlv>       - use dlv as a domain for DNSSEC look-aside validation
    -r <filename>  - read analysis from a file, instead of querying servers (use "-" for stdin)
    -t <threads>   - use multiple threads for analysis
    -a <ancestor>  - analyze ancestry of name through ancestor, instead of just name itself (use "-a ." or full ancestral analysis) (default: analyze only the name itself)
    -p             - make json output pretty instead of minimal
    -y             - read in yaml, instead of json
    -Y             - write in yaml, instead of json
    -o <filename>  - write the analysis to file instead of to stdout
''' % sys.argv[0])

def main():
    try:
        opts, args = getopt.getopt(sys.argv[1:], 'f:d:l:r:t:pyYo:a:')
    except getopt.GetoptError:
        usage()
        sys.exit(1)

    opts = dict(opts)
    if ('-f' in opts and args) or not ('-f' in opts or args):
        usage()
        sys.exit(1)

    if '-a' in opts:
        ceiling = dns.name.from_text(opts['-a'])
    else:
        ceiling = None

    if '-l' in opts:
        dlv_domain = dns.name.from_text(opts['-l'])
    else:
        dlv_domain = None

    try:
        processes = int(opts.get('-t', 1))
    except ValueError:
        usage()
        sys.exit(1)
    if processes < 1:
        usage()
        sys.exit(1)

    try:
        val = int(opts.get('-d', 1))
    except ValueError:
        usage()
        sys.exit(1)

    if val < 0 or val > 3:
        usage()
        sys.exit(1)

    if val > 2:
        debug_level = logging.DEBUG
    elif val > 1:
        debug_level = logging.INFO
    elif val > 0:
        debug_level = logging.WARNING
    else:
        debug_level = logging.ERROR
    handler = logging.StreamHandler()
    handler.setLevel(debug_level)
    logger.addHandler(handler)
    logger.setLevel(debug_level)

    if '-f' in opts:
        names = []
        with codecs.open(opts['-f'], 'r', 'utf-8') as f:
            for line in f:
                names.append(dns.name.from_unicode(line.strip()))
    else:
        names = map(dns.name.from_text, args)

    name_objs = []
    if '-r' in opts:
        if opts['-r'] == '-':
            analysis_str = sys.stdin.read()
        else:
            analysis_str = open(opts['-r']).read()
        if '-y' in opts:
            import yaml
            analysis_structured = yaml.load(analysis_str)
        else:
            analysis_structured = json.loads(analysis_str)
        for name in names:
            name_objs.append(DomainNameAnalysis.deserialize(name, analysis_structured))
    else:
        if '-t' in opts:
            a = ParallelAnalyst(ceiling, dlv_domain, processes)
        else:
            a = BulkAnalyst(ceiling, dlv_domain)
        name_objs = a.analyze(names)

    d = collections.OrderedDict()
    for name_obj in name_objs:
        if name_obj is None:
            continue
        name_obj.serialize(d)

    if '-p' in opts:
        kwargs = { 'indent': 4, 'separators': (',', ': ') }
    else:
        kwargs = {}

    if '-o' not in opts or opts['-o'] == '-':
        fh = sys.stdout
    else:
        fh = open(opts['-o'], 'w')

    if '-Y' in opts:
        import yaml
        fh.write(yaml.dump(d))
    else:
        fh.write(json.dumps(d, **kwargs))

if __name__ == "__main__":
    main()
